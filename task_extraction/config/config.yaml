
dataset:
  log_file: "data/Linux.txt"
project:
  PROJECT_ID : "log-analysis-433902"
  # LOCATION : "us-central1"
  LOCATION : "asia-east1"
gemini:
  MODEL_ID : "gemini-1.5-pro"
  LOCATION : "asia-east1"
  temperature: 0.2
  top_p : 0.8
  top_k : 32
  candidate_count : 1
  max_output_tokens : 8192
llama:
  MODEL_ID : "meta/llama3-405b-instruct-maas"  # ["meta/llama3-8b-instruct-maas", "meta/llama3-70b-instruct-maas", "meta/llama3-405b-instruct-maas"]
  LOCATION : "us-central1"
  apply_llama_guard : False  
  temperature : 0.2
  max_tokens : 4096  
  top_p : 0.8 
  stream : False
claude:
  MODEL_ID : "claude-3-5-sonnet@20240620" #"claude-3-opus@20240229"  # ["claude-3-5-sonnet@20240620", "claude-3-opus@20240229", "claude-3-haiku@20240307", "claude-3-sonnet@20240229" ]
  LOCATION : "us-east5"  #"europe-west1"
  max_tokens: 4096
  temperature: 0.2
  top_p : 0.8
  top_k : 32
mistral:
  MODEL_ID : "mistral-large-2"  
  MODEL_VERSION : "2407"
  is_streamed : False
  LOCATION : "us-central1"
inference:
  save_path: "result"
  chunk_size: 128*1024
test:
  llm_result_file: "result/result_2024-09-03_14-46-46_192k file1_counts/inference_output/output0.json"
  re_result_file: "data/splited data/12-192x1024/new_result_human_Linux_1.json"
  human_result_file: "data/splited data/12-192x1024/new_result_human_Linux_1.json"
comment: "192k file1 count "

